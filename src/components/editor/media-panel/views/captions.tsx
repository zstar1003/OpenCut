import { Button } from "@/components/ui/button";
import { PropertyGroup } from "../../properties-panel/property-item";
import { PanelBaseView as BaseView } from "@/components/editor/panel-base-view";
import { Language, LanguageSelect } from "@/components/language-select";
import { useState, useRef, useCallback } from "react";
import { extractTimelineAudio } from "@/lib/mediabunny-utils";
import { useTimelineStore } from "@/stores/timeline-store";
import { DEFAULT_TEXT_ELEMENT } from "@/constants/text-constants";
import { Loader2, Download, Cpu } from "lucide-react";
import { Progress } from "@/components/ui/progress";
import { TextElement } from "@/types/timeline";
import { useProjectStore } from "@/stores/project-store";

export const languages: Language[] = [
  { code: "chinese", name: "ä¸­æ–‡", flag: "CN" },
  { code: "english", name: "English", flag: "US" },
  { code: "japanese", name: "æ—¥æœ¬èª", flag: "JP" },
  { code: "korean", name: "í•œêµ­ì–´", flag: "KR" },
  { code: "spanish", name: "EspaÃ±ol", flag: "ES" },
  { code: "french", name: "FranÃ§ais", flag: "FR" },
  { code: "german", name: "Deutsch", flag: "DE" },
  { code: "russian", name: "Ğ ÑƒÑÑĞºĞ¸Ğ¹", flag: "RU" },
  { code: "portuguese", name: "PortuguÃªs", flag: "PT" },
  { code: "italian", name: "Italiano", flag: "IT" },
];

// Singleton for transcriber to avoid reloading
let transcriberInstance: any = null;
let transcriberLoading = false;
let transcriberLoadPromise: Promise<any> | null = null;
let currentModelId: string | null = null;

// è¿‡æ»¤æ— ç”¨å­—å¹•å†…å®¹çš„å‡½æ•°
const filterCaptionText = (text: string): string | null => {
  if (!text) return null;

  const trimmed = text.trim();

  // è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹ï¼ˆå•ä¸ªå­—ç¬¦æˆ–ç©ºç™½ï¼‰
  if (trimmed.length < 2) return null;

  // è¿‡æ»¤éŸ³ä¹/è§†é¢‘å…ƒæ•°æ®æ¨¡å¼
  const metadataPatterns = [
    /^[\(ï¼ˆ\[ã€]?(?:ç·¨æ›²|ç¼–æ›²|ä½œè©|ä½œè¯|ä½œæ›²|å­—å¹•|ç¿»è¯‘|ç¿»è­¯|æ··éŸ³|åˆ¶ä½œ|è£½ä½œ|ç›‘åˆ¶|ç›£åˆ¶|é…éŸ³|æ¼”å”±|åŸå”±|æ­Œè¯|æ­Œè©|MV|å¯¼æ¼”|å°æ¼”|æ‘„å½±|æ”å½±|å‰ªè¾‘|å‰ªè¼¯)[:ï¼š]?.+[\)ï¼‰\]ã€‘]?$/i,
    /^[\(ï¼ˆ\[ã€].+[:ï¼š].+[\)ï¼‰\]ã€‘]$/, // æ‹¬å·å†…å¸¦å†’å·çš„å†…å®¹
    /^(?:è¯|æ›²|ç¼–|æ··|åˆ¶|å”±)[:ï¼š]/,
    /^[A-Za-z\s]+[:ï¼š]/, // è‹±æ–‡åå­—åè·Ÿå†’å·
    /^â™ª+$|^â™«+$|^ğŸµ+$/, // çº¯éŸ³ä¹ç¬¦å·
    /^\[.*\]$/, // æ–¹æ‹¬å·å†…å®¹ [Music] ç­‰
    /^[\(ï¼ˆ].*[\)ï¼‰]$/, // ä»…æ‹¬å·å†…å®¹
  ];

  for (const pattern of metadataPatterns) {
    if (pattern.test(trimmed)) {
      console.log("Filtered metadata:", trimmed);
      return null;
    }
  }

  // ç§»é™¤æ–‡æœ¬ä¸­çš„å†…åµŒå…ƒæ•°æ®ï¼ˆä½†ä¿ç•™å…¶ä»–å†…å®¹ï¼‰
  let cleaned = trimmed
    .replace(/[\(ï¼ˆ\[ã€][^ï¼‰\)ã€‘\]]*(?:ç·¨æ›²|ç¼–æ›²|ä½œè©|ä½œè¯|ä½œæ›²|å­—å¹•|ç¿»è¯‘|ç¿»è­¯|æ··éŸ³|åˆ¶ä½œ|è£½ä½œ)[^ï¼‰\)ã€‘\]]*[\)ï¼‰\]ã€‘]/g, "")
    .replace(/\s+/g, " ")
    .trim();

  // è¿‡æ»¤æ¸…ç†åå¤ªçŸ­çš„å†…å®¹
  if (cleaned.length < 2) return null;

  return cleaned;
};

export function Captions() {
  // Default to Chinese
  const [selectedCountry, setSelectedCountry] = useState("chinese");
  const [isProcessing, setIsProcessing] = useState(false);
  const [processingStep, setProcessingStep] = useState<string>("");
  const [error, setError] = useState<string | null>(null);
  const [modelProgress, setModelProgress] = useState<number>(0);
  const [isLoadingModel, setIsLoadingModel] = useState(false);
  const containerRef = useRef<HTMLDivElement>(null);
  const { insertTrackAt, addElementToTrack } = useTimelineStore();
  const activeProject = useProjectStore((s) => s.activeProject);

  // æ¨¡å‹é…ç½® - ä½¿ç”¨ distil-whisper-large-v3ï¼ˆè’¸é¦ç‰ˆï¼‰
  // å‡†ç¡®åº¦æ¥è¿‘ large-v3ï¼Œä½†ä½“ç§¯æ›´å°ï¼ˆ~750MBï¼‰ï¼Œé€‚åˆæµè§ˆå™¨è¿è¡Œ
  const modelId = "Xenova/distil-whisper-large-v3";
  // å¤‡ç”¨æ¨¡å‹ï¼ˆå¦‚æœå†…å­˜ä¸è¶³åˆ™å›é€€åˆ°æ›´å°çš„æ¨¡å‹ï¼‰
  const fallbackModelId = "Xenova/whisper-small";

  // Track the highest progress value to prevent progress bar from going backwards
  const maxProgressRef = useRef(0);

  const loadTranscriber = useCallback(async () => {
    // If model changed, reset the instance
    if (currentModelId !== modelId && currentModelId !== fallbackModelId) {
      transcriberInstance = null;
      transcriberLoadPromise = null;
    }

    if (transcriberInstance) {
      return transcriberInstance;
    }

    if (transcriberLoading && transcriberLoadPromise) {
      return transcriberLoadPromise;
    }

    transcriberLoading = true;
    setIsLoadingModel(true);
    maxProgressRef.current = 0;

    transcriberLoadPromise = (async () => {
      try {
        // åŠ¨æ€å¯¼å…¥ transformers åº“
        const transformersModule = await import("@xenova/transformers");
        const { pipeline, env } = transformersModule;

        // é…ç½® transformers.js ç¯å¢ƒ
        if (env) {
          env.allowLocalModels = false;
          env.useBrowserCache = true;
        }

        const progressCallback = (progress: any) => {
          if (progress.status === "downloading" || progress.status === "progress") {
            const percent = progress.progress || 0;
            if (percent > maxProgressRef.current) {
              maxProgressRef.current = percent;
              setModelProgress(Math.round(percent));
            }
          } else if (progress.status === "ready" || progress.status === "done") {
            maxProgressRef.current = 100;
            setModelProgress(100);
          }
        };

        // å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœ HuggingFace ä¸å¯ç”¨åˆ™å°è¯• HF é•œåƒ
        const tryLoadModel = async (model: string) => {
          // é¦–å…ˆå°è¯•é»˜è®¤æºï¼ˆHuggingFaceï¼‰
          try {
            console.log(`å°è¯•ä» HuggingFace åŠ è½½æ¨¡å‹: ${model}`);
            if (env) {
              env.remoteHost = "https://huggingface.co";
              env.remotePathTemplate = "{model}/resolve/{revision}/";
            }
            return await pipeline("automatic-speech-recognition", model, {
              progress_callback: progressCallback,
            });
          } catch (hfError) {
            console.warn("HuggingFace åŠ è½½å¤±è´¥ï¼Œå°è¯• HF é•œåƒ...", hfError);

            // å°è¯• HF é•œåƒï¼ˆé€‚ç”¨äºä¸­å›½å¤§é™†ç”¨æˆ·ï¼‰
            try {
              maxProgressRef.current = 0;
              setModelProgress(0);
              if (env) {
                env.remoteHost = "https://hf-mirror.com";
                env.remotePathTemplate = "{model}/resolve/{revision}/";
              }
              console.log(`å°è¯•ä» HF é•œåƒåŠ è½½æ¨¡å‹: ${model}`);
              return await pipeline("automatic-speech-recognition", model, {
                progress_callback: progressCallback,
              });
            } catch (mirrorError) {
              console.warn("HF é•œåƒåŠ è½½å¤±è´¥", mirrorError);
              throw mirrorError;
            }
          }
        };

        // å°è¯•åŠ è½½ä¸»æ¨¡å‹
        let transcriber;
        try {
          console.log("åŠ è½½ ASR æ¨¡å‹:", modelId);
          transcriber = await tryLoadModel(modelId);
          currentModelId = modelId;
        } catch (primaryError) {
          // å¦‚æœä¸»æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹
          console.warn(`ä¸»æ¨¡å‹ ${modelId} åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹...`, primaryError);
          maxProgressRef.current = 0;
          setModelProgress(0);

          try {
            console.log("åŠ è½½å¤‡ç”¨æ¨¡å‹:", fallbackModelId);
            transcriber = await tryLoadModel(fallbackModelId);
            currentModelId = fallbackModelId;
          } catch (fallbackError) {
            throw new Error(`æ¨¡å‹åŠ è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–å°è¯•ä½¿ç”¨ VPN è®¿é—®ã€‚`);
          }
        }

        transcriberInstance = transcriber;
        return transcriber;
      } catch (err) {
        transcriberLoading = false;
        transcriberLoadPromise = null;
        throw err;
      } finally {
        transcriberLoading = false;
        setIsLoadingModel(false);
      }
    })();

    return transcriberLoadPromise;
  }, []);

  const handleGenerateTranscript = async () => {
    try {
      setIsProcessing(true);
      setError(null);
      setModelProgress(0);

      setProcessingStep("æ­£åœ¨åŠ è½½ AI æ¨¡å‹...");
      const transcriber = await loadTranscriber();

      setProcessingStep("æ­£åœ¨æå–éŸ³é¢‘...");
      const audioBlob = await extractTimelineAudio();
      console.log("Audio blob size:", audioBlob.size, "bytes");

      // Create a blob URL for the audio
      const audioUrl = URL.createObjectURL(audioBlob);
      console.log("Audio URL created:", audioUrl);

      setProcessingStep("æ­£åœ¨è¯†åˆ«è¯­éŸ³ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...");

      console.log("Transcription model:", currentModelId, "language:", selectedCountry);

      // Whisper æ¨¡å‹å‚æ•°
      const result = await transcriber(audioUrl, {
        return_timestamps: true,
        chunk_length_s: 30,
        stride_length_s: 5,
        language: selectedCountry,
        task: "transcribe",
      });

      // Revoke the blob URL
      URL.revokeObjectURL(audioUrl);

      console.log("Transcription completed:", result);

      setProcessingStep("æ­£åœ¨ç”Ÿæˆå­—å¹•è½¨é“...");

      const shortCaptions: Array<{
        text: string;
        startTime: number;
        duration: number;
      }> = [];

      let globalEndTime = 0;

      // Handle chunks with timestamps from transformers.js
      if (result.chunks && Array.isArray(result.chunks)) {
        for (const chunk of result.chunks) {
          const rawText = chunk.text?.trim();
          if (!rawText) continue;

          // è¿‡æ»¤æ— ç”¨å†…å®¹
          const filteredText = filterCaptionText(rawText);
          if (!filteredText) continue;

          const startTime = chunk.timestamp?.[0] ?? globalEndTime;
          const endTime = chunk.timestamp?.[1] ?? startTime + 2;
          const duration = Math.max(0.8, endTime - startTime);

          // å¯¹äºä¸­æ–‡ï¼ŒæŒ‰å­—ç¬¦åˆ†å‰²ï¼›å¯¹äºå…¶ä»–è¯­è¨€ï¼ŒæŒ‰ç©ºæ ¼åˆ†å‰²
          const isChinese = /[\u4e00-\u9fff]/.test(filteredText);
          let textChunks: string[] = [];

          if (isChinese) {
            // ä¸­æ–‡ï¼šæ¯ 4-6 ä¸ªå­—ç¬¦ä¸€ç»„
            const chars = filteredText.replace(/\s+/g, "");
            const chunkSize = 5;
            for (let i = 0; i < chars.length; i += chunkSize) {
              const chunk = chars.slice(i, i + chunkSize);
              if (chunk.length > 0) {
                textChunks.push(chunk);
              }
            }
          } else {
            // å…¶ä»–è¯­è¨€ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œæ¯ 3 ä¸ªè¯ä¸€ç»„
            const words = filteredText.split(/\s+/);
            for (let i = 0; i < words.length; i += 3) {
              textChunks.push(words.slice(i, i + 3).join(" "));
            }
          }

          if (textChunks.length === 0) {
            textChunks = [filteredText];
          }

          const chunkDuration = duration / textChunks.length;
          let chunkStartTime = startTime;

          for (const chunkText of textChunks) {
            // å†æ¬¡è¿‡æ»¤æ¯ä¸ªå°å—
            const finalText = filterCaptionText(chunkText);
            if (!finalText) continue;

            let adjustedStartTime = chunkStartTime;
            if (adjustedStartTime < globalEndTime) {
              adjustedStartTime = globalEndTime;
            }

            shortCaptions.push({
              text: finalText,
              startTime: adjustedStartTime,
              duration: Math.max(0.8, chunkDuration),
            });

            globalEndTime = adjustedStartTime + Math.max(0.8, chunkDuration);
            chunkStartTime += chunkDuration;
          }
        }
      } else if (result.text) {
        // Fallback for simple text output without timestamps
        const filteredText = filterCaptionText(result.text);
        if (filteredText) {
          const isChinese = /[\u4e00-\u9fff]/.test(filteredText);
          const defaultDuration = 2;

          if (isChinese) {
            const chars = filteredText.replace(/\s+/g, "");
            const chunkSize = 5;
            for (let i = 0; i < chars.length; i += chunkSize) {
              const chunk = chars.slice(i, i + chunkSize);
              const finalText = filterCaptionText(chunk);
              if (finalText) {
                shortCaptions.push({
                  text: finalText,
                  startTime: globalEndTime,
                  duration: defaultDuration,
                });
                globalEndTime += defaultDuration;
              }
            }
          } else {
            const words = filteredText.split(/\s+/);
            for (let i = 0; i < words.length; i += 3) {
              const chunkText = words.slice(i, i + 3).join(" ");
              const finalText = filterCaptionText(chunkText);
              if (finalText) {
                shortCaptions.push({
                  text: finalText,
                  startTime: globalEndTime,
                  duration: defaultDuration,
                });
                globalEndTime += defaultDuration;
              }
            }
          }
        }
      }

      if (shortCaptions.length === 0) {
        throw new Error("æœªæ£€æµ‹åˆ°éŸ³é¢‘ä¸­çš„è¯­éŸ³");
      }

      // Create a single track for all captions
      const captionTrackId = insertTrackAt("text", 0);

      // Calculate y position for bottom center (standard subtitle position)
      const captionY = activeProject?.canvasSize?.height
        ? activeProject.canvasSize.height * 0.4 // ~90% from top, lower position
        : 350; // Default for 1080p canvas

      // Add all caption elements to the same track
      for (let index = 0; index < shortCaptions.length; index++) {
        const caption = shortCaptions[index];
        addElementToTrack(captionTrackId, {
          ...DEFAULT_TEXT_ELEMENT,
          name: `å­—å¹• ${index + 1}`,
          content: caption.text,
          duration: caption.duration,
          startTime: caption.startTime,
          fontSize: 48,
          fontWeight: "bold",
          y: captionY,
          strokeColor: "#000000",
          strokeWidth: 2,
        } as TextElement);
      }

      console.log(
        `${shortCaptions.length} caption chunks added to timeline!`
      );
    } catch (error) {
      console.error("Transcription failed:", error);
      setError(
        error instanceof Error ? error.message : "å‘ç”Ÿäº†æ„å¤–é”™è¯¯"
      );
    } finally {
      setIsProcessing(false);
      setProcessingStep("");
    }
  };

  return (
    <BaseView ref={containerRef} className="flex flex-col justify-between h-full">
      <div className="space-y-4">
        <PropertyGroup title="è¯­è¨€">
          <LanguageSelect
            selectedCountry={selectedCountry}
            onSelect={setSelectedCountry}
            containerRef={containerRef}
            languages={languages}
          />
        </PropertyGroup>

        <div className="p-3 bg-muted/50 rounded-md space-y-2">
          <div className="flex items-center gap-2 text-sm text-muted-foreground">
            <Cpu className="h-4 w-4" />
            <span>åœ¨æµè§ˆå™¨æœ¬åœ°è¿è¡Œ</span>
          </div>
          <p className="text-xs text-muted-foreground">
            ä½¿ç”¨ distil-whisper-large-v3 æ¨¡å‹ï¼ˆè’¸é¦ç‰ˆï¼‰ï¼Œå‡†ç¡®åº¦é«˜ä¸”å†…å­˜å‹å¥½ã€‚é¦–æ¬¡ä½¿ç”¨éœ€ä¸‹è½½çº¦ 750MB æ¨¡å‹æ–‡ä»¶ã€‚æ”¯æŒ HF é•œåƒè‡ªåŠ¨åˆ‡æ¢ã€‚
          </p>
        </div>
      </div>

      <div className="flex flex-col gap-4">
        {error && (
          <div className="p-3 bg-destructive/10 border border-destructive/20 rounded-md">
            <p className="text-sm text-destructive">{error}</p>
          </div>
        )}

        {isLoadingModel && modelProgress > 0 && modelProgress < 100 && (
          <div className="space-y-2">
            <div className="flex items-center gap-2 text-sm text-muted-foreground">
              <Download className="h-4 w-4" />
              <span>æ­£åœ¨ä¸‹è½½ AI æ¨¡å‹... {modelProgress}%</span>
            </div>
            <Progress value={modelProgress} className="h-2" />
          </div>
        )}

        <Button
          className="w-full"
          onClick={handleGenerateTranscript}
          disabled={isProcessing}
        >
          {isProcessing && <Loader2 className="mr-1 h-4 w-4 animate-spin" />}
          {isProcessing ? processingStep : "ç”Ÿæˆå­—å¹•"}
        </Button>
      </div>
    </BaseView>
  );
}
